{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le script synthèse et traduction du LADIREC\n",
    "Il utilise le cadre de travail de Hugging face ou sont déposés des modèles de langues pour tiré de différentes stratégies.\n",
    "Ce script utilise les modèles de type BERT et applique les contraintes de ce type de modèle (longeur maximale, type de lemmatisation etc.).\n",
    "Il est possible de modifier le modèle utilisé à même le code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le premier bloc doit être roulé en mode administrateur\n",
    "# Installation des librairies et packages nécessaire au script\n",
    "# Cette opération ne devrait être exécutée qu'une seule fois (ou à même le terminal), mais peut-être effectuer plusieurs foies sans conséquence.\n",
    "!python -m pip install --upgrade pip setuptools notebook jupyter\n",
    "\n",
    "# https://pytorch.org/get-started/locally/ to get the pyTorch command line to install on you local machine use this link\n",
    "# the command provider here is for windows/pip/python/CUDA11.6\n",
    "# link to download the latest windows CUDA toolkit https://developer.nvidia.com/cuda-downloads\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install transformers[sentencepiece] sacremoses\n",
    "\n",
    "!pip install pandas openpyxl\n",
    "\n",
    "# Documentation de Pipeline sur huggingFace : https://huggingface.co/docs/transformers/v4.22.1/en/quicktour#pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ev-EHzgVNwK",
    "outputId": "c89764eb-6b45-4022-d313-0aa3b694d179"
   },
   "outputs": [],
   "source": [
    "# Import des librairies\n",
    "import pandas as pd\n",
    "import time\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déclaration des variables globales, constantes et paramêtres du traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_CORPUS contient le chemin et le nom du fichier original du corpus - format xlsx demandé\n",
    "# Cette valeur est une constante et ne devrait pas changer\n",
    "PATH_CORPUS = \"corpus_copie_fr_short.xlsx\"\n",
    "\n",
    "# LISTE_COLONNE correspond à la liste des noms des colonnes sélectionnées pour les analyses\n",
    "# !cette valeur est une constante et ne devrait pas changer\n",
    "LISTE_COLONNE = ['T1','U1']\n",
    "\n",
    "# LANGUE_OG_CORPUS correspond à la langue original du corpus de texte lors du READ - \n",
    "# Cette valeur est une constante et ne devrait pas changer en cours de traitement\n",
    "LANGUE_OG_CORPUS = 'FR'\n",
    "\n",
    "# LANGUE_CHAINE dresse la liste des langues pour la traduction en chaine (traduction dans une/des langues, puis retour à la langue originale LANGUE_OG_CORPUS)\n",
    "# Cette valeur est une constante et ne devrait pas changer\n",
    "LANGUE_CHAINE = ['ES', 'EN']\n",
    "\n",
    "# utiliser comme paramètre de summerizer pour la longueur maximum du résumé\n",
    "SYNTHESE_MAX_LENGTH = 80\n",
    "\n",
    "# nombre de mot dans un chunk\n",
    "CHUNK_SIZE = 250\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Déclaration des fonctions utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du corpus\n",
    "# la fonction qui importe et créé le dataset à partir du corpus\n",
    "def get_dataset(path, col = None):\n",
    "    df1 = pd.read_excel(path, usecols=col)\n",
    "    #par défaut, toutes le colonnes de la table sont chargé en df1\n",
    "\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des colonnes à analyser\n",
    "# la fonction qui fusionne des colonnes selon une liste sélectionnée dans la variable globale LISTE_COLONNE.\n",
    "def merge_col(df, colSelect):\n",
    "\n",
    "    df_merged = pd.DataFrame(columns=['raw'])\n",
    "    df_merged['raw'] = df[colSelect].apply(\n",
    "        lambda row: (\" \".join(row.values.astype(str))), axis=1\n",
    "    )\n",
    "    # # Ajouté : df_merged[raw] qui contient la fusion des colonnes sélectionnées sans le retrait des majuscules, le df retourné contient 2 col.\n",
    "    # df_merged['line'] = df[colSelect].apply(\n",
    "    #     lambda row: (\" \".join(row.values.astype(str)).lower()), axis=1\n",
    "    # )\n",
    "\n",
    "    # display(df_merged)\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Début du traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHxpnx82VhHP",
    "outputId": "6b758a37-6bd0-4f61-a8c2-5c139417d238"
   },
   "outputs": [],
   "source": [
    "# chargement de données (corpus)\n",
    "# MAIN\n",
    "print(PATH_CORPUS)\n",
    "start_time = time.time()\n",
    "\n",
    "data_origin = get_dataset(PATH_CORPUS)\n",
    "data = merge_col(data_origin, LISTE_COLONNE)\n",
    "data[\"Doc_ID\"] = data_origin[\"Doc_ID\"]\n",
    "\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk the raw data in segment of CHUNK_SIZE mots\n",
    "from cgitb import text\n",
    "\n",
    "\n",
    "def text_chunking(data, chunk_size)\n",
    "    start_time = time.time()\n",
    "\n",
    "    nb_word_series = []\n",
    "    raw_series = data['raw']\n",
    "    raw_chunk_series = []\n",
    "    chunks = []\n",
    "    continue_chunking = True\n",
    "\n",
    "    for index, segment in raw_series.items():\n",
    "        nb_word_series.append(len(segment.split(' ')))\n",
    "        # print(f\"Nombre de mot {index}: {len(segment.split(' '))}\")\n",
    "\n",
    "        while continue_chunking:\n",
    "            tokens = segment.split(\" \")\n",
    "            if len(tokens) >= CHUNK_SIZE:\n",
    "                index_dernier_point = \" \".join(tokens[:CHUNK_SIZE]).rfind(\".\") + 1\n",
    "                if index_dernier_point > 0:\n",
    "                    chunks.append(segment[:index_dernier_point])\n",
    "                    segment = segment[index_dernier_point:]\n",
    "                else:\n",
    "                    chunks.append(\" \".join(tokens[:CHUNK_SIZE]))\n",
    "                    segment = \" \".join(tokens[CHUNK_SIZE:])\n",
    "            else:\n",
    "                chunks.append(segment)\n",
    "                continue_chunking = False\n",
    "\n",
    "        raw_chunk_series.append(chunks)\n",
    "\n",
    "        chunks = []\n",
    "        continue_chunking = True\n",
    "\n",
    "    # ajouter les text chunker dans une nouvel colonne du df\n",
    "    data[\"raw_chunk\"] = raw_chunk_series\n",
    "    data[\"nb_word\"] = nb_word_series\n",
    "\n",
    "    print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "    display(data)\n",
    "return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chunk the raw data in segment of CHUNK_SIZE mots\n",
    "start_time = time.time()\n",
    "\n",
    "nb_word_series = []\n",
    "raw_series = data['raw']\n",
    "raw_chunk_series = []\n",
    "chunks = []\n",
    "continue_chunking = True\n",
    "\n",
    "for index, segment in raw_series.items():\n",
    "    nb_word_series.append(len(segment.split(' ')))\n",
    "    # print(f\"Nombre de mot {index}: {len(segment.split(' '))}\")\n",
    "\n",
    "    while continue_chunking:\n",
    "        tokens = segment.split(\" \")\n",
    "        if len(tokens) >= CHUNK_SIZE:\n",
    "            index_dernier_point = \" \".join(tokens[:CHUNK_SIZE]).rfind(\".\") + 1\n",
    "            if index_dernier_point > 0:\n",
    "                chunks.append(segment[:index_dernier_point])\n",
    "                segment = segment[index_dernier_point:]\n",
    "            else:\n",
    "                chunks.append(\" \".join(tokens[:CHUNK_SIZE]))\n",
    "                segment = \" \".join(tokens[CHUNK_SIZE:])\n",
    "        else:\n",
    "            chunks.append(segment)\n",
    "            continue_chunking = False\n",
    "\n",
    "    raw_chunk_series.append(chunks)\n",
    "\n",
    "    chunks = []\n",
    "    continue_chunking = True\n",
    "\n",
    "# ajouter les text chunker dans une nouvel colonne du df\n",
    "data[\"raw_chunk\"] = raw_chunk_series\n",
    "data[\"nb_word\"] = nb_word_series\n",
    "\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le corpus nettoyé dans un fichier excel\n",
    "# à faire : réordonner les colonnes\n",
    "# + ajouter une variable globale de noms de colonnes du dataset original à ajouter aux sorties à titre de parametrage\n",
    "\n",
    "data.to_excel('corpus_nettoyé.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche des entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nouveau Pipeline avec hugging face à exploiter ici\n",
    "# DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Synthétiser des textes en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\",\n",
    "                       model=\"moussaKam/barthez-orangesum-title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# roule la synthese sur chaque chunk de chaque text\n",
    "data['synthese'] = data['raw_chunk'].apply(\n",
    "    lambda row: ([summarizer(chunk, max_length=SYNTHESE_MAX_LENGTH) for chunk in row])\n",
    ")\n",
    "\n",
    "# remettre ensemble les syntheses comme un grand string dans une seule colones\n",
    "data['synthese'] = data['synthese'].apply(\n",
    "    lambda row: (\" | \".join([chunk[0][\"summary_text\"] for chunk in row]))\n",
    ")\n",
    "\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation du pipeline de traduction de français à anglais\n",
    "fr_to_en = pipeline(\"translation_fr_to_en\", \n",
    "                    model=\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# roule la traduction de français à anglais sur chaque chunk de chaque texte\n",
    "data['fr_to_en'] = data['raw_chunk'].apply(\n",
    "    lambda row: (fr_to_en(row))\n",
    ")\n",
    "\n",
    "# remettre ensemble les syntheses comme un grand string dans une seule colones\n",
    "data['fr_to_en'] = data['fr_to_en'].apply(\n",
    "    lambda row: (\" | \".join([chunk['translation_text'] for chunk in row]))\n",
    ")\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du pipeline de traduction de anglais à français\n",
    "en_to_fr = pipeline(\"translation_en_to_fr\", \n",
    "                    model=\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# roule la traduction de anglais à français sur chaque chunk de chaque texte\n",
    "\n",
    "data['en_to_fr'] = data['fr_to_en'].apply(\n",
    "    lambda row: (en_to_fr(row))\n",
    ")\n",
    "\n",
    "# remettre ensemble les traductions comme un grand string dans une seule colones\n",
    "data['en_to_fr'] = data['en_to_fr'].apply(\n",
    "    lambda row: (\" | \".join([chunk['translation_text'] for chunk in row]))\n",
    ")\n",
    "print(\"\\n  >> temps d\\'execution : {:.2f} s\".format(round(time.time() - start_time, 2)))\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Export final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du dataframe sous forme MS Excel \n",
    "# réordonner les colonnes\n",
    "# ajouter une liste de nom de colonnes du DF original à ajouter à la sortie corpus enrichi\n",
    "\n",
    "data.to_excel('corpus_enrichi.xlsx')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Acfas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
